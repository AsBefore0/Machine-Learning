## 机器学习算法总结

### 决策算法

#### 决策树

[https://sklearn.apachecn.org/](https://sklearn.apachecn.org/)

#### 随机森林

### 降维算法

#### PCA降维

### 聚类算法

#### K-means算法

无监督学习

K-means聚类通过最小化样本到其所属簇中心的距离来实现数据的分组。具体而言，K-means的目标是将数据分成K个簇，并使每个簇中的数据点到其质心（centroid）的欧氏距离平方和最小。

$$
J = \sum_{k=1}^{K} \sum_{x \in S_k} || x - C_k ||^2
$$

算法步骤

1. **初始化** ：随机选择K个数据点作为初始质心。
2. **分配簇** ：对于数据集中的每个数据点，计算其到各个质心的距离，并将其分配到距离最近的质心所在的簇。
3. **更新质心** ：对于每个簇，计算所有分配到该簇的数据点的平均值，更新该簇的质心。
4. **重复** ：重复步骤2和3，直到质心不再发生显著变化，或者达到预设的迭代次数。

时间复杂度为 $O(n \cdot K \cdot t)$，其中n是数据点数量，K是簇的数量，t是迭代次数。

**注意事项**

1. **需要预设K值** ：必须提前确定簇的数量K，且K值的选择对结果影响较大。
2. **对初始质心敏感** ：初始质心的选择会影响最终结果，可能会陷入局部最优。
3. **对噪声和异常值敏感** ：噪声和异常值可能会严重影响簇的结果。

选择K值通常根据肘部法则

绘制不同K值下的SSE（误差平方和）曲线，选择拐点作为K值

### 分类算法

#### KNN算法

监督学习

适用于分类和回归问题

它的核心思想是根据距离度量来判断数据点的类别或预测其数值，即给定一个测试样本，找到训练集中与该样本最近的K个样本，并根据这些样本的类别或数值做出预测。

具体步骤

1. 对于每个测试样本，计算它与所有训练样本之间的距离。常用的距离度量有欧几里得距离、曼哈顿距离等。
   $$
   d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
   $$
2. 按照计算得到的距离，对训练样本排序，选出距离测试样本最近的K个邻居。
3. 对于分类问题，K个最近邻的类别进行投票，少数服从多数，票数最多的类别作为测试样本的预测类别。
4. 对于回归问题，计算K个最近邻的数值的平均值，作为测试样本的预测值

注意事项

KNN需要计算测试样本与所有训练样本之间的距离，在数据量大时计算开销较大。

K值的选择对算法性能影响很大，通常需要通过**交叉验证**来选择合适的K值。

#### 支持向量机

### 朴素贝叶斯

### 线性回归

### 逻辑回归
