### 机器学习算法总结

#### 优化算法

参考的python算法库

[https://scikit-opt.github.io/scikit-opt/#/zh/README](https://scikit-opt.github.io/scikit-opt/#/zh/README)

##### 差分进化算法 DE

初始化    变异     交叉    选择

###### 变异操作

基础的变异方法

$$
V_i =X_1 + F*(X_2 - X_3)
$$

下标是随机选取的三个个体

F过小会降低收敛速度   F过大会导致种群不收敛

###### 交叉操作

对变异后的个体 $X_i$ 和 $V_i$ 进行操作  生成一个试验向量

$$
U_{ij}=\begin{cases}
V_{ij}, & if \quad rand(0,1) \le CR \quad or \quad j = j_{rand}\\
X_{ij}, & otherwise
\end{cases}
$$

其中$CR$是交叉概率, $j_{rand}$是一个随机选择的维度索引，确保至少有一个维度来自$V_i$

###### 选择操作

比较实验向量 $U_i$ 和 $X_i$ 谁的效果比较好  就留下谁

##### 遗传算法 DA

初始化种群   适应度评估  选择操作  交叉操作  编译操作  选择下一代

###### 选择操作

选择操作通过选择适应度高的个体来生成下一代，常用的方法有轮盘赌选择

概率的表示方式为：

$$
P_i = \frac{f(X_i)}{\sum_{j=1}^{N_p} f(X_j))}
$$

###### 交叉操作

交叉操作是模拟基因重组的过程，生成新的个体。常见的交叉方式有单点交叉和多点交叉。

对于单点交叉来说  选择交叉点为K

$$
X_1' = (x_{11}, x_{12}, \ldots, x_{1k}, x_{21}, x_{22}, \ldots, x_{2N})
$$

$$
X_2' = (x_{21}, x_{22}, \ldots, x_{2k}, x_{11}, x_{12}, \ldots, x_{1N})
$$

###### 变异操作

变异操作用于引入新的基因，以避免种群陷入局部最优

对于实数编码，变异可以通过加上一个小的随机扰动来实现：

$$
X_{ij}=x_{ij}+N(0,σ^2)
$$

###### 选择操作

根据适应度，选择生成的新个体形成下一代种群。

可以通过保留一定比例的 `精英个体`来保留最佳解。

##### 粒子群优化算法 PSO

每个解被看作一个“粒子”，`每个粒子`都表示问题解空间中的一个候选解。

粒子群在解空间中飞行时，会根据其自身的历史经验和群体的整体经验

来调整自己的飞行 `方向`和 `速度`，以达到最优解。

两个核心概念：个体最优位置 `pbest`和全局最优位置 `gbest`。

###### 速度更新

$$
v_{i}(t+1) = \omega v_{i}(t) + c_1 r_1 (pbest_i - x_i(t)) + c_2 r_2 (gbest - x_i(t))
$$

$w$为惯性权重，控制粒子移动的惯性；

较大的惯性权重有利于全局搜索,较小的惯性权重有利于局部搜索,通常使用动态调整策略。

$c_1,c_2$是学习因子,分别控制着向全局最优靠近还是个体最优靠近

$r_1,r_2$是取自0到1之间的随机数

###### 位置更新

$$
x_{i}(t+1) = x_{i}(t) + v_{i}(t+1)
$$

更新个体最优位置和全局最优位置

根据当前粒子的位置和适应度值，更新 `pbest`和 `gbest`。

###### 参数优化策略

对于惯性权重   先设置较大的值利于全局搜索 再设置较小的值进行局部搜索

$$
\omega(t) = \omega_{\text{max}} - \frac{\omega_{\text{max}} - \omega_{\text{min}}}{T_{\text{max}}} \cdot t
$$

对于 $c_1, c_2$。逐渐脱离个体最优向全局最优靠拢

$$
c_1(t) = c_{1\text{max}} - \frac{c_{1\text{max}} - c_{1\text{min}}}{T_{\text{max}}} \cdot t
$$

$$
c_2(t) = c_{2\text{min}} + \frac{c_{2\text{max}} - c_{2\text{min}}}{T_{\text{max}}} \cdot t
$$

###### 特点

* PSO不需要复杂的编码或交叉变异操作，易于实现。
* 能较快地收敛到全局最优解。
* 容易并行化以提高效率。

##### 蚁群优化算法

* **信息素更新** ：蚂蚁在经过路径时会留下信息素，路径的选择概率依赖于信息素的浓度。
* **状态转移规则** ：蚂蚁通过信息素和启发式信息（例如距离或其他问题相关信息）来选择下一步的路径。
* **信息素挥发** ：信息素会随着时间逐渐挥发，防止过度依赖某条路径，增强探索能力。

###### 初始化

设置蚂蚁数量、最大迭代次数、信息素初始值等参数。

###### 蚂蚁构造解

每只蚂蚁根据概率选择下一步路径，直到完成一个完整解。

状态转移概率公式

$$
p_{ij}^k(t) = \frac{\left[\tau_{ij}(t)\right]^\alpha \cdot \left[\eta_{ij}\right]^\beta}{\sum_{l \in \mathcal{N}_i^k} \left[\tau_{il}(t)\right]^\alpha \cdot \left[\eta_{il}\right]^\beta}
$$

$\tau_{ij}(t)$ 表示从节点 $i$ 到节点 $j$ 的信息素浓度。

$\eta_{ij}$ 是启发式信息，通常是节点 $i$ 到节点  $j$ 的距离的倒数

$$
\eta_{ij} = \frac{1}{d_{ij}}
$$

$\alpha$ 是信息素的重要性参数，$\beta$ 是启发式信息的重要性参数。

###### 更新信息素

根据蚂蚁找到的解更新信息素，包括信息素挥发和增量。

$$
\tau_{ij}(t+1) = (1 - \rho) \cdot \tau_{ij}(t) + \sum_{k=1}^{m} \Delta \tau_{ij}^k(t)
$$

$\rho$ 是信息素挥发系数，控制信息素的挥发速度

$\Delta \tau_{ij}^k(t)$ 是蚂蚁 $k$ 在时间对路径 $(i ,j)$留下的信息素增量，具体公式为：

$$
\Delta \tau_{ij}^k(t) = 
\begin{cases}
\frac{Q}{L_k}, & \text{if ant } k \text{ uses edge } (i, j) \\
0, & \text{otherwise}
\end{cases}

$$



$Q$ 是常数，通常与问题规模相关。

$L_k$ 是蚂蚁 $k$ 的路径长度，即蚂蚁在一轮中走过的路径的总长度。

###### 迭代

重复构造解和更新信息素的过程，直到满足终止条件。

###### 特点

* $\alpha$ : 控制蚂蚁对信息素的依赖程度。当 $\alpha$ 越大，蚂蚁更倾向于选择信息素浓度高的路径。
* $\beta$ : 控制蚂蚁对启发式信息的依赖程度。当 $\beta$ 越大，蚂蚁更倾向于选择路径短的节点。

##### 模拟退火算法  SA

模拟退火算法的最大优势是其可以跳出局部最优解，从而有机会找到全局最优解。

在高温状态下，系统可以接受较差的解，这样可以避免陷入局部最优；

而在低温状态下，系统趋向于接受较好的解，从而收敛到全局最优解。

###### 初始化

确定初始解 $x_0$，设定初始温度 $T_0$，以及降温系数 $\alpha$和终止温度 $T_{min}$

计算初始解的目标函数值

$$
E_{old} = f(x_0)
$$

###### 循环过程

在当前解的邻域内随机生成一个新解 $x_{new}$

计算新解的目标函数值 $E_{new} = f(x_{new})$

新的候选解是否被接受取决于当前解和候选解之间的差异，以及当前温度 $T$

其被接受的概率公式如下所示：

$$
P(E_{new}, E_{old}, T) = \begin{cases} 
1, & if \quad E_{new} \leq E_{old} \\
\exp\left(\frac{-(E_{new} - E_{old})}{T}\right), & if \quad E_{new} > E_{old}
\end{cases}
$$

随着温度的降低，系统趋向于接受更优的解。

更新解和温度

* 如果接受了新解，则将当前解更新为新解 $x_{old} = x_{new}$
* 温度的更新公式如下

$$
T_{k+1} = \alpha T_k
$$

###### 降温策略

逐步降低温度，直到达到预定的终止条件。

通常为达到最低温度$T_{min}$或者在若干次迭代中没有找到更优解。

###### 变种

Fast模拟退火   Boltzmann模拟退火    Cauchy模拟退火

它们主要区别在于 `温度下降策略`和 `候选解生成策略`

**Fast**

超几何降温策略

$$
T_k = \frac{T_0}{1 + k}
$$

这种降温方式使得算法初期的温度下降较快，从而快速探索解空间

后期逐渐趋于平缓，以更细致地搜索全局最优解

采用随机扰动生成新的候选解

$$
x_{new} = x_{old} + \Delta x
$$

**Boltzmann**

$$
T_k = \frac{T_0}{\ln(1 + k)}
$$

这种对数降温方式在前期的降温速度相对较慢，

使得算法有更多机会进行全局搜索, 适合多峰问题

使用正态分布生成候选解

$$
x_{new} = x_{old} + \sigma \cdot \mathcal{N}(0, 1)
$$

**Cauchy**

超几何降温策略

$$
T_k = \frac{T_0}{1 + k}
$$

使用 `Cauchy分布`生成候选解

$$
x_{new} = x_{old} + \gamma \cdot Cauchy(0, 1)
$$

与正态分布不同，Cauchy分布生成的新解可能具有较大的偏差

这有助于算法跳出局部最优,适合搜索范围较大的问题

#### 决策算法

##### 决策树

[https://sklearn.apachecn.org/](https://sklearn.apachecn.org/)

#### 降维算法

##### PCA降维

#### 聚类算法

##### K-means算法
